# original  python franka_socket_test.py  --ckpt-path /home/csl/lerobot/outputs/train/0725_red_gripper_change_action/checkpoints/last/pretrained_model/ --eval-freq 10
import socket
import os
import struct
import time
import torch
import numpy as np
from datetime import datetime
import cv2
import argparse
import json
from lerobot.common.policies.act.modeling_act import ACTPolicy


class ACTInferenceServer:
    def __init__(self, ckpt_path, host='0.0.0.0', port=5001, device='cuda', eval_freq=None):
        self.eval_freq = eval_freq  # In Hz
        self.eval_interval = 1.0 / eval_freq if eval_freq and eval_freq > 0 else 0  # seconds

        # 1. Device setup
        self.device = device
        if device == 'cuda' and not torch.cuda.is_available():
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œè‡ªå‹•åˆ‡æ›ç‚º CPU")
            self.device = 'cpu'

        print("Using:", self.device)
        ckpt_path = os.path.abspath(ckpt_path)
        print(f"ğŸ“‚ ä½¿ç”¨çš„ ckpt_path: {ckpt_path}")

        try:
            self.policy = ACTPolicy.from_pretrained(
                ckpt_path,
                local_files_only=True
            )
        except Exception as e:
            raise RuntimeError(f"è¼‰å…¥ ACTPolicy å¤±æ•—ï¼Œè«‹ç¢ºèª ckpt è·¯å¾‘åŠæª”æ¡ˆï¼š{e}")

        self.policy.to(self.device)
        self.policy.eval()

        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_socket.bind((host, port))
        self.server_socket.listen(1)
        print("ğŸš€ ACT æ¨è«–ä¼ºæœå™¨å·²å•Ÿå‹•ï¼Œç­‰å¾… client é€£ç·š...")
        self.conn, self.addr = self.server_socket.accept()
        print("âœ… client å·²é€£ç·š:", self.addr)

    def recv_data(self):
        start_time = time.time()
        raw_len = self.conn.recv(4)
        if not raw_len:
            return None, None, 0, 0
        data_len = struct.unpack('>I', raw_len)[0]
        data_type = self.conn.recv(10).decode().strip()
        data = b''
        while len(data) < data_len:
            packet = self.conn.recv(data_len - len(data))
            if not packet:
                break
            data += packet
        end_time = time.time()
        return data_type, data, data_len, end_time - start_time

    def send_data(self, data_type, data_bytes):
        data_type = data_type.ljust(10).encode()
        data_len = struct.pack('>I', len(data_bytes))
        self.conn.sendall(data_len + data_type + data_bytes)

    def run(self):
        last_time = time.time()
        while True:
            try:
                loop_start_time = time.time()
                processing_start_time = loop_start_time

                type1, data1, size1, time1 = self.recv_data()
                if type1 != 'list':
                    continue
                state = json.loads(data1.decode())
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)

                type2, data2, size2, time2 = self.recv_data()
                if type2 != 'img1':
                    continue
                img1 = cv2.imdecode(np.frombuffer(data2, np.uint8), cv2.IMREAD_COLOR)

                type3, data3, size3, time3 = self.recv_data()
                if type3 != 'img2':
                    continue
                img2 = cv2.imdecode(np.frombuffer(data3, np.uint8), cv2.IMREAD_COLOR)

                obs = {
                    'observation.state': state_tensor,
                    'observation.images.image': torch.from_numpy(img1).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0,
                    'observation.images.image_additional_view': torch.from_numpy(img2).permute(2, 0, 1).unsqueeze(0).float().to(self.device) / 255.0
                }

                inference_start_time = time.time()
                with torch.no_grad():
                    action = self.policy.select_action(obs)
                inference_end_time = time.time()

                action_np = action.squeeze(0).cpu().numpy().tolist()

                current_time = time.time()
                total_interval = current_time - last_time
                last_time = current_time
                total_freq = 1.0 / total_interval if total_interval > 0 else 0
                inference_freq = 1.0 / (inference_end_time - inference_start_time) if inference_end_time > inference_start_time else 0
                processing_interval = inference_start_time - processing_start_time
                total_bytes = size1 + size2 + size3
                total_transfer_time = time1 + time2 + time3
                transfer_speed = total_bytes / total_transfer_time if total_transfer_time > 0 else 0

                print(f"æ¨è«–å®Œæˆ: {np.round(action_np[7],6)} | "
                      f"ç¸½é »ç‡: {total_freq:.2f} Hz | "
                      f"ç´”æ¨è«–é »ç‡: {inference_freq:.2f} Hz | "
                      f"è³‡æ–™è™•ç†è€—æ™‚: {processing_interval:.4f} s | "
                      f"å‚³è¼¸é€Ÿç‡: {transfer_speed / 1024 / 1024:.2f} MB/s")

                self.send_data('list', json.dumps(action_np).encode())

                # --- æ§åˆ¶æ¨è«–é »ç‡ ---
                if self.eval_interval > 0:
                    elapsed = time.time() - loop_start_time
                    sleep_time = self.eval_interval - elapsed
                    if sleep_time > 0:
                        time.sleep(sleep_time)

            except Exception as e:
                print("âŒ ç™¼ç”ŸéŒ¯èª¤:", e)
                break

        self.conn.close()
        self.server_socket.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--ckpt-path', type=str, required=True, help='Path to pretrained_model folder')
    parser.add_argument('--port', type=int, default=5001, help='Port number')
    parser.add_argument('--device', type=str, default='cuda', help='Device to run inference')
    parser.add_argument('--eval-freq', type=float, default=0, help='Evaluation frequency in Hz (0 for max speed)')
    args = parser.parse_args()

    server = ACTInferenceServer(
        ckpt_path=args.ckpt_path,
        port=args.port,
        device=args.device,
        eval_freq=args.eval_freq
    )
    server.run()


# import socket
# import os
# import struct
# import time
# import torch
# import numpy as np
# from datetime import datetime
# import cv2
# from lerobot.common.policies.act.modeling_act import ACTPolicy


# class ACTInferenceServer:
#     def __init__(self, ckpt_path, host='0.0.0.0', port=5001, device='cuda'):
#         # 1. è¨­å®šè£ç½®
#         self.device = device
#         if device == 'cuda' and not torch.cuda.is_available():
#             print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œè‡ªå‹•åˆ‡æ›ç‚º CPU")
#             self.device = 'cpu'

#         # 2. çµ•å°è·¯å¾‘è½‰æ›
#         print("Using:",self.device)
#         ckpt_path = os.path.abspath(ckpt_path)
#         print(f"ğŸ“‚ ä½¿ç”¨çš„ ckpt_path: {ckpt_path}")

#         # 3. ä½¿ç”¨å®˜æ–¹æ¨è–¦æ–¹å¼è¼‰å…¥ config.json èˆ‡æ¬Šé‡
#         #    éœ€ç¢ºä¿ pretrained_model è³‡æ–™å¤¾åº•ä¸‹æœ‰ config.json å’Œ model.safetensors
#         try:
#             self.policy = ACTPolicy.from_pretrained(
#                 ckpt_path,
#                 local_files_only=True
#             )
#         except Exception as e:
#             raise RuntimeError(f"è¼‰å…¥ ACTPolicy å¤±æ•—ï¼Œè«‹ç¢ºèª ckpt è·¯å¾‘åŠæª”æ¡ˆï¼š{e}")

#         # 4. ç§»åˆ°è£ç½®ä¸¦è¨­ç‚º eval
#         self.policy.to(self.device)
#         self.policy.eval()


#         # 5. å»ºç«‹ socket server
#         self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
#         self.server_socket.bind((host, port))
#         self.server_socket.listen(1)
#         print("ğŸš€ ACT æ¨è«–ä¼ºæœå™¨å·²å•Ÿå‹•ï¼Œç­‰å¾… client é€£ç·š...")
#         self.conn, self.addr = self.server_socket.accept()
#         print("âœ… client å·²é€£ç·š:", self.addr)

#     def recv_data(self):
#         start_time = time.time()
#         raw_len = self.conn.recv(4)
#         if not raw_len:
#             return None, None, 0, 0
#         data_len = struct.unpack('>I', raw_len)[0]
#         data_type = self.conn.recv(10).decode().strip()
#         data = b''
#         while len(data) < data_len:
#             packet = self.conn.recv(data_len - len(data))
#             if not packet:
#                 break
#             data += packet
#         end_time = time.time()
#         return data_type, data, data_len, end_time - start_time



#     def send_data(self, data_type, data_bytes):
#         data_type = data_type.ljust(10).encode()
#         data_len = struct.pack('>I', len(data_bytes))
#         self.conn.sendall(data_len + data_type + data_bytes)

#     def run(self):
#         last_time = time.time()
#         while True:
#             try:
#                 # --- æ­¥é©Ÿ 1: æ¸¬é‡æ¥æ”¶èˆ‡å‰è™•ç†æ™‚é–“ ---
#                 processing_start_time = time.time()

#                 # æ¥æ”¶ç‹€æ…‹ list
#                 type1, data1, size1, time1 = self.recv_data()
#                 if type1 != 'list':
#                     continue
#                 state = json.loads(data1.decode())
#                 state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)

#                 # æ¥æ”¶å½±åƒ img1
#                 type2, data2, size2, time2 = self.recv_data()
#                 if type2 != 'img1':
#                     continue
#                 img1 = cv2.imdecode(np.frombuffer(data2, np.uint8), cv2.IMREAD_COLOR)

#                 # æ¥æ”¶å½±åƒ img2
#                 type3, data3, size3, time3 = self.recv_data()
#                 if type3 != 'img2':
#                     continue
#                 img2 = cv2.imdecode(np.frombuffer(data3, np.uint8), cv2.IMREAD_COLOR)

#                 # å»ºç«‹è§€æ¸¬ dict
#                 obs = {
#                     'observation.state': state_tensor,
#                     'observation.images.image': torch.from_numpy(img1).permute(2,0,1).unsqueeze(0).float().to(self.device) / 255.0,
#                     'observation.images.image_additional_view': torch.from_numpy(img2).permute(2,0,1).unsqueeze(0).float().to(self.device) / 255.0
#                 }

#                 # --- æ­¥é©Ÿ 2: ç²¾ç¢ºæ¸¬é‡æ¨¡å‹æ¨è«–æ™‚é–“ ---
#                 inference_start_time = time.time()
#                 with torch.no_grad():
#                     action = self.policy.select_action(obs)
#                 inference_end_time = time.time()

#                 action_np = action.squeeze(0).cpu().numpy().tolist()

#                 # --- æ­¥é©Ÿ 3: è¨ˆç®—èˆ‡è¼¸å‡ºå„ç¨®æ™‚é–“èˆ‡æ•ˆèƒ½æŒ‡æ¨™ ---
#                 current_time = time.time()
#                 total_interval = current_time - last_time
#                 last_time = current_time
#                 total_freq = 1.0 / total_interval if total_interval > 0 else 0

#                 inference_interval = inference_end_time - inference_start_time
#                 inference_freq = 1.0 / inference_interval if inference_interval > 0 else 0

#                 processing_interval = inference_start_time - processing_start_time

#                 # å‚³è¼¸é€Ÿåº¦ (Byte/s -> MB/s)
#                 total_bytes = size1 + size2 + size3
#                 total_transfer_time = time1 + time2 + time3
#                 transfer_speed = total_bytes / total_transfer_time if total_transfer_time > 0 else 0  # Byte/s

#                 print(f"æ¨è«–å®Œæˆ: {np.round(action_np[7],6)} | "
#                     f"ç¸½é »ç‡: {total_freq:.2f} Hz | "
#                     f"ç´”æ¨è«–é »ç‡: {inference_freq:.2f} Hz | "
#                     f"è³‡æ–™è™•ç†è€—æ™‚: {processing_interval:.4f} s | "
#                     f"å‚³è¼¸é€Ÿç‡: {transfer_speed/1024/1024:.2f} MB/s")

#                 # --- æ­¥é©Ÿ 4: å‚³é€æ¨è«–çµæœå› client ---
#                 self.send_data('list', json.dumps(action_np).encode())

#             except Exception as e:
#                 print("âŒ ç™¼ç”ŸéŒ¯èª¤:", e)
#                 break

#         self.conn.close()
#         self.server_socket.close()


# if __name__ == '__main__':
#     import json
#     ckpt_path = '/home/csl/lerobot/outputs/train/0725_red_gripper_change_action/checkpoints/last/pretrained_model'
#     server = ACTInferenceServer(ckpt_path, port=5001, device='cuda')
#     server.run()



# import socket
# import os
# import struct
# import time
# import torch
# import numpy as np
# import cv2
# import json # ç¢ºä¿ json è¢« import
# from lerobot.common.policies.act.modeling_act import ACTPolicy


# class ACTInferenceServer:
#     def __init__(self, ckpt_path, host='0.0.0.0', port=5001, device='cuda'):
#         # ... (é€™éƒ¨åˆ†èˆ‡ä½ åŸæœ¬çš„ç¨‹å¼ç¢¼ç›¸åŒï¼Œä¿æŒä¸è®Š)
#         # 1. è¨­å®šè£ç½®
#         self.device = device
#         if device == 'cuda' and not torch.cuda.is_available():
#             print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œè‡ªå‹•åˆ‡æ›ç‚º CPU")
#             self.device = 'cpu'

#         # 2. çµ•å°è·¯å¾‘è½‰æ›
#         print("Using:",self.device)
#         ckpt_path = os.path.abspath(ckpt_path)
#         print(f"ğŸ“‚ ä½¿ç”¨çš„ ckpt_path: {ckpt_path}")

#         # 3. è¼‰å…¥æ¨¡å‹
#         try:
#             self.policy = ACTPolicy.from_pretrained(
#                 ckpt_path,
#                 local_files_only=True
#             )
#         except Exception as e:
#             raise RuntimeError(f"è¼‰å…¥ ACTPolicy å¤±æ•—ï¼Œè«‹ç¢ºèª ckpt è·¯å¾‘åŠæª”æ¡ˆï¼š{e}")

#         # 4. ç§»åˆ°è£ç½®ä¸¦è¨­ç‚º eval
#         self.policy.to(self.device)
#         self.policy.eval()

#         # 5. å»ºç«‹ socket server
#         self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        
#         # [å„ªåŒ–å»ºè­° 1] è¨­å®š TCP_NODELAY ä»¥é™ä½å»¶é²
#         self.server_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        
#         self.server_socket.bind((host, port))
#         self.server_socket.listen(1)
#         print("ğŸš€ ACT æ¨è«–ä¼ºæœå™¨å·²å•Ÿå‹•ï¼Œç­‰å¾… client é€£ç·š...")
#         self.conn, self.addr = self.server_socket.accept()
        
#         # ç•¶ client é€£ç·šå¾Œä¹Ÿè¨­å®š TCP_NODELAY
#         self.conn.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        
#         print("âœ… client å·²é€£ç·š:", self.addr)

#     def _recv_all(self, n):
#         """ä¸€å€‹è¼”åŠ©å‡½å¼ï¼Œç¢ºä¿æ¥æ”¶åˆ°æŒ‡å®šé•·åº¦çš„æ•¸æ“š"""
#         data = b''
#         while len(data) < n:
#             packet = self.conn.recv(n - len(data))
#             if not packet:
#                 return None
#             data += packet
#         return data

#     def recv_observation_data(self):
#         """
#         ä¸€æ¬¡æ€§æ¥æ”¶æ‰€æœ‰è§€æ¸¬è³‡æ–™ (state, img1, img2)
#         å”å®š: [4 bytes state_len][4 bytes img1_len][4 bytes img2_len][state_data][img1_data][img2_data]
#         """
#         # 1. æ¥æ”¶åŒ…å«ä¸‰å€‹é•·åº¦è³‡è¨Šçš„æ¨™é ­ (4+4+4 = 12 bytes)
#         header = self._recv_all(12)
#         if not header:
#             return None, None, None, 0
        
#         state_len, img1_len, img2_len = struct.unpack('>III', header)
        
#         # 2. æ ¹æ“šæ¨™é ­ä¸­çš„ç¸½é•·åº¦ï¼Œä¸€æ¬¡æ€§æ¥æ”¶æ‰€æœ‰å‰©ä¸‹çš„è³‡æ–™
#         total_data_len = state_len + img1_len + img2_len
#         all_data = self._recv_all(total_data_len)
#         if not all_data:
#             return None, None, None, 0
            
#         # 3. æ ¹æ“šé•·åº¦åˆ‡å‰²è³‡æ–™
#         state_data = all_data[:state_len]
#         img1_data = all_data[state_len : state_len + img1_len]
#         img2_data = all_data[state_len + img1_len :]
        
#         return state_data, img1_data, img2_data, total_data_len

#     def send_data(self, data_type, data_bytes):
#         # ... (é€™å€‹å‡½å¼ä¿æŒä¸è®Š)
#         data_type = data_type.ljust(10).encode()
#         data_len = struct.pack('>I', len(data_bytes))
#         self.conn.sendall(data_len + data_type + data_bytes)

#     def run(self):
#         last_time = time.time()
#         while True:
#             try:
#                 # --- æ­¥é©Ÿ 1: æ¸¬é‡æ¥æ”¶èˆ‡å‰è™•ç†æ™‚é–“ ---
#                 recv_start_time = time.time()
                
#                 # æ”¹ç‚ºå‘¼å«æ–°çš„å–®æ¬¡æ¥æ”¶å‡½å¼
#                 state_data, img1_data, img2_data, total_bytes_received = self.recv_observation_data()

#                 if state_data is None:
#                     print("Client ç«¯å·²æ–·é–‹é€£ç·šã€‚")
#                     break
                
#                 recv_end_time = time.time()

#                 # --- è³‡æ–™å‰è™•ç† ---
#                 processing_start_time = time.time()

#                 # è§£ç¢¼èˆ‡è½‰æ›
#                 state = json.loads(state_data.decode())
#                 state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)

#                 img1 = cv2.imdecode(np.frombuffer(img1_data, np.uint8), cv2.IMREAD_COLOR)
#                 img2 = cv2.imdecode(np.frombuffer(img2_data, np.uint8), cv2.IMREAD_COLOR)

#                 # å»ºç«‹è§€æ¸¬ dict
#                 obs = {
#                     'observation.state': state_tensor,
#                     'observation.images.image': torch.from_numpy(img1).permute(2,0,1).unsqueeze(0).float().to(self.device) / 255.0,
#                     'observation.images.image_additional_view': torch.from_numpy(img2).permute(2,0,1).unsqueeze(0).float().to(self.device) / 255.0
#                 }
                
#                 processing_end_time = time.time()

#                 # --- æ­¥é©Ÿ 2: ç²¾ç¢ºæ¸¬é‡æ¨¡å‹æ¨è«–æ™‚é–“ ---
#                 inference_start_time = time.time()
#                 with torch.no_grad():
#                     action = self.policy.select_action(obs)
#                 inference_end_time = time.time()

#                 action_np = action.squeeze(0).cpu().numpy().tolist()

#                 # --- æ­¥é©Ÿ 3: è¨ˆç®—èˆ‡è¼¸å‡ºå„ç¨®æ™‚é–“èˆ‡æ•ˆèƒ½æŒ‡æ¨™ ---
#                 current_time = time.time()
#                 total_interval = current_time - last_time
#                 last_time = current_time
#                 total_freq = 1.0 / total_interval if total_interval > 0 else 0

#                 recv_interval = recv_end_time - recv_start_time
#                 processing_interval = processing_end_time - processing_start_time
#                 inference_interval = inference_end_time - inference_start_time
                
#                 transfer_speed = total_bytes_received / recv_interval if recv_interval > 0 else 0

#                 print(f"æ¨è«–å®Œæˆ: {np.round(action_np[7],6)} | "
#                       f"ç¸½é »ç‡: {total_freq:.2f} Hz | "
#                       f"ç´”æ¨è«–é »ç‡: {inference_freq:.2f} Hz | "
#                       f"æ¥æ”¶è€—æ™‚: {recv_interval:.4f} s | "
#                       f"å‰è™•ç†è€—æ™‚: {processing_interval:.4f} s | "
#                       f"å‚³è¼¸é€Ÿç‡: {transfer_speed/1024/1024:.2f} MB/s")

#                 # --- æ­¥é©Ÿ 4: å‚³é€æ¨è«–çµæœå› client ---
#                 self.send_data('list', json.dumps(action_np).encode())

#             except (ConnectionResetError, BrokenPipeError) as e:
#                 print(f"âŒ Client é€£ç·šä¸­æ–·: {e}")
#                 break
#             except Exception as e:
#                 print(f"âŒ ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
#                 import traceback
#                 traceback.print_exc()
#                 break

#         print("ğŸ›‘ ä¼ºæœå™¨æ­£åœ¨é—œé–‰...")
#         self.conn.close()
#         self.server_socket.close()

# if __name__ == '__main__':
#     ckpt_path = '/home/csl/lerobot/outputs/train/0721_change_action/checkpoints/last/pretrained_model'
#     server = ACTInferenceServer(ckpt_path, port=5001, device='cuda')
#     server.run()